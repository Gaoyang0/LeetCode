### 优化器

**梯度下降法**
在微积分中，对多元函数的参数$\theta$求偏导数，把求得的各个参数的导数以向量的形式写出来就是梯度。梯度就是函数变化最快的地方。梯度下降是迭代法的一种，在求解机器学习算法的模型参数$\theta$时，即无约束问题时，梯度下降是最常采用的方法之一。顾名思义，梯度下降法的计算过程就是沿梯度下降的方向求解极小值，也可以沿梯度上升方向求解最大值。(BGD,SGD,mini-batch)

**动量优化法**

动量优化方法引入物理学中的动量思想，**加速梯度下降**，有Momentum和Nesterov两种算法。当我们将一个小球从山上滚下来，没有阻力时，它的动量会越来越大，但是如果遇到了阻力，速度就会变小，动量优化法就是借鉴此思想，使得梯度方向在不变的维度上，参数更新变快，梯度有所改变时，更新参数变慢，这样就能够加快收敛并且减少动荡。

**Momentum**

momentum算法思想：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来加速当前的梯度。假设 ![[公式]](https://www.zhihu.com/equation?tex=m_%7Bt%7D) 表示t时刻的动量， ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 表示动量因子，通常取值0.9或者近似值，在SGD的基础上增加动量，则参数更新公式如下：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+m_%7Bt%2B1%7D%3D%5Cmu+%5Ccdot+m_%7Bt%7D+%2B+%5Calpha+%5Ccdot+%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+%5Cright%29+%5C%5C+%5Ctheta_%7Bt%2B1%7D%3D%5Ctheta_%7Bt%7D+-+m_%7Bt%2B1%7D)

在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。总而言之，momentum能够加速SGD收敛，抑制震荡。

理解：

1. 在上一次梯度的基础上做改变
2. 用加速来理解，当加速度一直是正的化，速度会一直变大，当有一定的速度，加速度突然反向，速度不会立马变负

**自适应学习率优化算法**

**AdaGrad**

定义参数：全局学习率$\delta$，一般会选择 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%3D0.01) ; 一个极小的常量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) ，通常取值10e-8,目的是为了避免分母为0; 梯度加速变量(gradient accumulation variable) ![[公式]](https://www.zhihu.com/equation?tex=r) 。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C++g+%5Cleftarrow++%5Ctriangledown_%7B%5Ctheta%7D+J+%5Cleft%28%5Ctheta+%5Cright%29+%5C%5C+++r+%5Cleftarrow+r+%2B+g%5E%7B2%7D+%5C%5C++%5Ctriangle+%5Ctheta+%5Cleftarrow++%5Cfrac%7B%5Cdelta+%7D%7B%5Csqrt%7Br+%2B+%5Cepsilon%7D%7D%5Ccdot+g+%5C%5C+%5Ctheta+%5Cleftarrow+%5Ctheta+-+%5Ctriangle+%5Ctheta+)

从上式可以看出，梯度加速变量r为t时刻前梯度的平方和 ![[公式]](https://www.zhihu.com/equation?tex=r+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bt%7D+g_%7Bi%7D%5E%7B2%7D) , 那么参数更新量$\triangle \theta$，将$1/\sqrt{r+\epsilon}$看成一个约束项regularizer. 在前期，梯度累计平方和比较小，也就是r相对较小，则约束项较大，这样就能够放大梯度, 参数更新量变大; 随着迭代次数增多，梯度累计平方和也越来越大，即r也相对较大，则约束项变小，这样能够缩小梯度，参数更新量变小。

**缺点：**

- 仍需要手工设置一个全局学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta) , 如果 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta) 设置过大的话，会使regularizer过于敏感，对梯度的调节太大
- 中后期，分母上梯度累加的平方和会越来越大，使得参数更新量趋近于0，使得训练提前结束，无法学习

**Adam: Adaptive Moment Estimation**

Adam中动量直接并入了梯度一阶矩（指数加权）的估计。其次，相比于缺少修正因子导致二阶矩估计可能在训练初期具有很高偏置的RMSProp，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩估计。 默认参数值设定为： ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_%7B1%7D+%3D+0.9%2C+%5Cbeta_%7B2%7D+%3D+0.999%2C+%5Cepsilon%3D10%5E%7B-8%7D)

![[公式]](https://www.zhihu.com/equation?tex=g+%5Cleftarrow+%5Ctriangledown_%7B%5Ctheta%7D+%5C%5C++J+%5Cleft%28%5Ctheta+%5Cright%29+%5C++m_%7Bt%7D+%5Cleftarrow+%5Cbeta_%7B1%7D+%5Ccdot+m_%7Bt-1%7D+%2B+%5Cleft%281+-+%5Cbeta_%7B1%7D+%5Cright%29+%5Ccdot+g_%7Bt%7D+%5C%5C++v_%7Bt%7D+%5Cleftarrow+%5Cbeta_%7B2%7D+%5Ccdot+v_%7Bt-1%7D+%2B+%5Cleft%28+1+-+%5Cbeta_%7B2%7D+%5Cright%29+%5Ccdot+g_%7Bt%7D%5E%7B2%7D++%5C%5C+%5Chat%7Bm%7D%7Bt%7D+%5Cleftarrow+%5Cfrac%7Bm%7Bt%7D%7D%7B1+-+%5Cbeta_%7B1%7D%5E%7Bt%7D%7D+%5C%5C++%5Chat%7Bv%7D%7Bt%7D+%5Cleftarrow+%5Cfrac%7Bv%7Bt%7D%7D%7B1+-+%5Cbeta_%7B2%7D%5E%7Bt%7D%7D+%5C%5C+%5Ctheta_%7Bt%2B1%7D+%3D+%5Ctheta_%7Bt%7D+-+%5Cfrac%7B%5Cdelta%7D%7B%5Cepsilon+%2B+%5Csqrt%7B%5Chat%7Bv_%7Bt%7D%7D%7D%7D+%5Ccdot+%5Chat%7Bm%7D_%7Bt%7D)

*其中，* ![[公式]](https://www.zhihu.com/equation?tex=m_%7Bt%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=n_%7Bt%7D) 分别是对梯度的一阶矩估计和二阶矩估计； ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bm%7D_%7Bt%7D) *，* ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bn%7D_%7Bt%7D) 是对 ![[公式]](https://www.zhihu.com/equation?tex=m_%7Bt%7D)， ![[公式]](https://www.zhihu.com/equation?tex=n_%7Bt%7D)的偏差校正，这样可以近似为对期望的无偏估计

**特点：**

- Adam梯度经过偏置校正后，每一次迭代学习率都有一个固定范围，使得参数比较平稳。
- 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
- 为不同的参数计算不同的自适应学习率
- 也适用于大多非凸优化问题——适用于大数据集和高维空间。
